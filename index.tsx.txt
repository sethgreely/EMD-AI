import React, { useState, useEffect, useRef, useCallback } from 'react';
import { createRoot } from 'react-dom/client';
import { GoogleGenAI, Chat } from "@google/genai";

// --- START: WEB SPEECH API TYPES ---
interface SpeechRecognitionAlternative {
    readonly transcript: string;
    readonly confidence: number;
}

interface SpeechRecognitionResult {
    readonly [index: number]: SpeechRecognitionAlternative;
    readonly length: number;
    readonly isFinal: boolean;
    item(index: number): SpeechRecognitionAlternative;
}

interface SpeechRecognitionResultList {
    readonly [index: number]: SpeechRecognitionResult;
    readonly length: number;
    item(index: number): SpeechRecognitionResult;
}

interface SpeechRecognitionEvent extends Event {
    readonly results: SpeechRecognitionResultList;
    readonly resultIndex: number;
}

interface SpeechRecognitionErrorEvent extends Event {
    readonly error: string;
    readonly message: string;
}

interface SpeechRecognition extends EventTarget {
    continuous: boolean;
    interimResults: boolean;
    lang: string;
    start(): void;
    stop(): void;
    abort(): void;
    onstart: (this: SpeechRecognition, ev: Event) => any;
    onend: (this: SpeechRecognition, ev: Event) => any;
    onerror: (this: SpeechRecognition, ev: SpeechRecognitionErrorEvent) => any;
    onresult: (this: SpeechRecognition, ev: SpeechRecognitionEvent) => any;
}

declare global {
    interface Window {
        SpeechRecognition: new () => SpeechRecognition;
        webkitSpeechRecognition: new () => SpeechRecognition;
    }
}
// --- END: WEB SPEECH API TYPES ---

// --- TYPES ---
type View = 'MAIN' | 'CALL' | 'LEARN' | 'AAR';
type Message = {
    sender: 'EMD' | 'USER' | 'SYSTEM';
    text: string;
};
type AARData = {
    stressTaps: number[]; // Timestamps in ms from call start
};

// --- PREMIUM VOICE SYNTHESIS ---
let audioQueue: HTMLAudioElement[] = [];
let isPlayingQueue = false;

// When deployed on Vercel, the app and the API live at the same domain.
// This relative path will work automatically.
const TTS_API_ENDPOINT = '/api/tts'; 

const playGoogleVoice = async (text: string): Promise<void> => {
    try {
        const response = await fetch(TTS_API_ENDPOINT, {
            method: 'POST',
            body: JSON.stringify({ text }),
        });
        if (!response.ok) {
            throw new Error(`TTS API failed with status ${response.status}`);
        }
        const data = await response.json();
        const audioContent = data.audioContent;
        const audio = new Audio(`data:audio/mp3;base64,${audioContent}`);
        audioQueue.push(audio);
        
        if (!isPlayingQueue) {
            processQueue();
        }
    } catch (error) {
        console.error("Failed to play Google voice:", error);
        // Fallback to robotic voice if API fails
        const utterance = new SpeechSynthesisUtterance("I'm having a connection issue. Please stand by.");
        window.speechSynthesis.speak(utterance);
    }
};

const processQueue = async () => {
    if (audioQueue.length === 0) {
        isPlayingQueue = false;
        return;
    }
    isPlayingQueue = true;
    const audio = audioQueue.shift();
    if (audio) {
        await new Promise<void>((resolve) => {
            audio.onended = () => resolve();
            audio.onerror = () => {
                console.error("Error playing audio.");
                resolve();
            }
            audio.play();
        });
    }
    processQueue();
};


// --- MAIN APP & VIEW ROUTER ---
const App: React.FC = () => {
    const [view, setView] = useState<View>('MAIN');
    const [aarData, setAarData] = useState<AARData | null>(null);

    useEffect(() => {
        // Stop all audio when view changes
        audioQueue = [];
        isPlayingQueue = false;
        document.querySelectorAll('audio').forEach(audio => audio.pause());
    }, [view]);

    const handleEndCall = (data: AARData) => {
        setAarData(data);
        setView('AAR');
    };

    const resetApp = () => {
        setAarData(null);
        setView('MAIN');
    };

    switch (view) {
        case 'CALL':
            return <CallScreen onEndCall={handleEndCall} />;
        case 'LEARN':
            return <LearnScreen onExit={() => setView('MAIN')} />;
        case 'AAR':
            return <AfterActionReportScreen data={aarData} onExit={resetApp} />;
        default:
            return (
                <MainScreen
                    onStartCall={() => setView('CALL')}
                    onLearn={() => setView('LEARN')}
                />
            );
    }
};

// --- SCREENS ---

const MainScreen: React.FC<{ onStartCall: () => void; onLearn: () => void; }> = ({ onStartCall, onLearn }) => (
    <main className="main-screen">
        <header className="main-header">
            <h1>Bystander</h1>
            <p>Practice your response in a simulated 911 call.</p>
        </header>
        <div className="emergency-button-container">
            <button className="emergency-button" onClick={onStartCall}>
                Call 911
            </button>
            <p className="emergency-button-label">This is a simulation. You will not be connected to actual emergency services.</p>
        </div>
        <div className="secondary-actions">
            <button className="secondary-button" onClick={onLearn}>
                Learn CPR & First Aid
            </button>
        </div>
    </main>
);

const LearnScreen: React.FC<{ onExit: () => void }> = ({ onExit }) => {
    return (
        <div className="learn-screen">
            <h1>Learn Lifesaving Skills</h1>
            <div className="module-list">
                <div className="module-card"><h2>Recognizing an Emergency</h2><p>Learn to identify signs of cardiac arrest, choking, and other crises.</p></div>
                <div className="module-card"><h2>Hands-Only CPR</h2><p>Master the simple but powerful technique of chest compressions.</p></div>
                <div className="module-card"><h2>Using an AED</h2><p>Understand how to operate an Automated External Defibrillator.</p></div>
                <div className="module-card"><h2>Overcoming Hesitation</h2><p>Explore the psychology of why people don't act and build confidence.</p></div>
            </div>
            <button className="secondary-button back-button" onClick={onExit}>Back to Main Menu</button>
        </div>
    );
};

const AfterActionReportScreen: React.FC<{ data: AARData | null; onExit: () => void }> = ({ data, onExit }) => {
    useEffect(() => {
        playGoogleVoice("Call ended. Let's review how you managed the stress of the call.");
    }, []);

    const getStressAnalysis = () => {
        if (!data || data.stressTaps.length === 0) {
            return "Excellent focus. You stayed engaged with the patient and dispatcher without being distracted by the arrival time. In a real crisis, this level of concentration is exactly what's needed.";
        }
        const tapCount = data.stressTaps.length;
        if (tapCount <= 3) {
            return `You checked on the ETA ${tapCount} time${tapCount > 1 ? 's' : ''}. This is a natural response to a high-stress situation, showing you were balancing awareness of incoming help with the immediate tasks. Good work.`;
        }
        return `The ETA became a frequent focus for you, with ${tapCount} checks. This is a common pattern when adrenaline is high, but it reveals a critical training point: every moment spent looking away is a moment not spent on the patient. The goal is to build trust in the system so you can focus 100% on the life in front of you.`;
    };

    return (
        <div className="aar-screen">
            <div className="flow-content">
                <h2>After Action Report</h2>
                <div className="report-section">
                    <h3>Psychological Insight: Stress Response</h3>
                    <p>{getStressAnalysis()}</p>
                </div>
                <p className="aar-summary">This simulation is designed to build muscle memory and confidence. Great job practicing.</p>
                <div className="action-buttons">
                    <button className="primary-action" onClick={onExit}>Return to Main Menu</button>
                </div>
            </div>
        </div>
    );
};


// --- 911 CALL SIMULATOR ---

const EMD_SYSTEM_INSTRUCTION = `You are an expert Emergency Medical Dispatcher (EMD) running a 911 call simulation. Your tone is calm, professional, and authoritative but reassuring. Your responses must be concise and direct.
1.  Your first and only initial response is EXACTLY: "911, what's the address of your emergency?"
2.  Wait for the user's response. Do not proceed until you get a location. If they are evasive, calmly repeat the need for a location.
3.  Once location is confirmed, ask "Okay, tell me exactly what happened."
4.  Listen to the user's description. Your goal is to determine the nature of the emergency (e.g., cardiac arrest, choking, trauma) and who the patient is. You can ask for the patient's name to make it more personal.
5.  Ask about the patient's status: "Are they conscious?", "Are they breathing?".
6.  Based on the answers, provide clear, numbered, one-at-a-time pre-arrival instructions (e.g., how to perform CPR, control bleeding). Wait for the user to acknowledge a step before giving the next one.
7.  **Curveball**: At an appropriate time, after starting instructions, introduce a challenge. For example, say "Is there an AED nearby? I need you to go find it while keeping me on the phone." or "I need you to check if their airway is clear. Tell me what you see.".
8.  Keep responses concise and direct. Use plain language. End the simulation by saying "Okay, help is on the way. I see they are arriving now. You did a great job. Stay on the line until they officially take over."`;

const CallScreen: React.FC<{ onEndCall: (data: AARData) => void }> = ({ onEndCall }) => {
    const [messages, setMessages] = useState<Message[]>([]);
    const [isProcessing, setIsProcessing] = useState(true);
    const [isListening, setIsListening] = useState(false);
    const [etaCurrent, setEtaCurrent] = useState<number | null>(null);
    const [showEta, setShowEta] = useState(false);
    const [stressTaps, setStressTaps] = useState<number[]>([]);
    const callStartTimeRef = useRef<number | null>(null);
    const chatRef = useRef<Chat | null>(null);
    const recognitionRef = useRef<SpeechRecognition | null>(null);
    const transcriptRef = useRef<HTMLDivElement>(null);
    const ambianceAudioRef = useRef<HTMLAudioElement | null>(null);

    // Scroll to bottom of transcript
    useEffect(() => {
        transcriptRef.current?.scrollTo({ top: transcriptRef.current.scrollHeight, behavior: 'smooth' });
    }, [messages]);

    // Initialize Call & ETA Timer
    useEffect(() => {
        ambianceAudioRef.current = document.getElementById('ambiance-audio') as HTMLAudioElement;

        try {
            const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });
            chatRef.current = ai.chats.create({
                model: 'gemini-2.5-flash',
                config: { systemInstruction: EMD_SYSTEM_INSTRUCTION },
            });
            
            const areaTypes = { 'Urban': [120, 300], 'Suburban': [300, 600], 'Rural': [600, 1200] };
            const randomArea = ['Urban', 'Suburban', 'Rural'][Math.floor(Math.random() * 3)] as keyof typeof areaTypes;
            const [min, max] = areaTypes[randomArea];
            const initialEta = Math.floor(Math.random() * (max - min + 1)) + min;
            setEtaCurrent(initialEta);

            setMessages([{ sender: 'SYSTEM', text: 'Connecting to 911...' }]);
            ambianceAudioRef.current?.play().catch(e => console.log("Ambiance audio play requires user interaction."));

            setTimeout(async () => {
                const firstMessage = "911, what's the address of your emergency?";
                await playGoogleVoice(firstMessage);
                setMessages(prev => [...prev, { sender: 'EMD', text: firstMessage }]);
                setIsProcessing(false);
                callStartTimeRef.current = Date.now();
            }, 1500);
        } catch (error) {
            console.error("Gemini AI initialization failed:", error);
            setMessages([{ sender: 'SYSTEM', text: 'Error: Could not connect to the simulation service.' }]);
        }

        const timer = setInterval(() => {
            setEtaCurrent(prev => (prev && prev > 0 ? prev - 1 : 0));
        }, 1000);

        return () => {
             if (recognitionRef.current) recognitionRef.current.abort();
             audioQueue = [];
             isPlayingQueue = false;
             ambianceAudioRef.current?.pause();
             clearInterval(timer);
        }
    }, []);

    // Setup Speech Recognition
    useEffect(() => {
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SpeechRecognition) {
            console.error("Speech Recognition not supported in this browser.");
            return;
        };
        const recognition = new SpeechRecognition();
        recognition.continuous = false;
        recognition.interimResults = false;
        recognition.lang = 'en-US';
        recognition.onstart = () => setIsListening(true);
        recognition.onend = () => setIsListening(false);
        recognition.onerror = (event) => {
            console.error("Speech recognition error:", event.error);
            setIsListening(false);
        };
        recognition.onresult = (event) => handleUserInput(event.results[0][0].transcript);
        recognitionRef.current = recognition;
    }, []);

    const handleUserInput = async (text: string) => {
        if (!text.trim() || !chatRef.current) return;
        setIsProcessing(true);
        setMessages(prev => [...prev, { sender: 'USER', text }]);

        try {
            const response = await chatRef.current.sendMessage({ message: text });
            const emdResponseText = response.text;
            playGoogleVoice(emdResponseText);
            setMessages(prev => [...prev, { sender: 'EMD', text: emdResponseText }]);
            if (emdResponseText.toLowerCase().includes("they are arriving now")) {
                setTimeout(() => onEndCall({ stressTaps }), 4000);
            }
        } catch (error) {
            console.error("Gemini API error:", error);
            const errorMessage = "I'm having trouble connecting. Please repeat that.";
            playGoogleVoice(errorMessage);
            setMessages(prev => [...prev, { sender: 'EMD', text: errorMessage }]);
        } finally {
            setIsProcessing(false);
        }
    };
    
    const toggleListen = () => {
        if (!recognitionRef.current) return;
        if (isListening) recognitionRef.current.stop();
        else recognitionRef.current.start();
    };

    const formatTime = (totalSeconds: number | null) => {
        if (totalSeconds === null) return '...';
        const minutes = Math.floor(totalSeconds / 60);
        const seconds = totalSeconds % 60;
        return `${minutes}:${seconds < 10 ? '0' : ''}${seconds}`;
    };

    const handleCheckEta = () => {
        if (callStartTimeRef.current) {
            const timeSinceStart = Date.now() - callStartTimeRef.current;
            setStressTaps(prev => [...prev, timeSinceStart]);
        }
        setShowEta(true);
        setTimeout(() => setShowEta(false), 2000);
    };

    return (
        <div className="call-screen">
            <header className="call-header">
                <div className="dynamic-island" onClick={handleCheckEta} role="button" tabIndex={0} aria-label="Check ambulance arrival time">
                    {showEta ? (
                         <div className="eta-display">
                            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M19.95 11.2c.23-.42.35-.9.35-1.42C20.3 7.95 19.18 6.5 17.75 6.5h-1.3V4.88c0-1.02-.83-1.88-1.88-1.88h-1.5c-.65 0-1.25.34-1.58.88L9.4 8.25H4.5C3.12 8.25 2 9.38 2 10.75v3.5c0 .68.28 1.28.7 1.72l1.62 1.83C4.7 18.25 5.3 18.5 6 18.5h8.38c.55 0 1.05-.28 1.33-.72l2.92-4.58c.2-.3.32-.63.32-1V12h-.5c-.45 0-.85-.3-.98-.73l-.62-2.1c-.2-.68.3-1.35 1-1.35h2.43c.4 0 .75.28.85.66l.35 1.28c.1.38.42.66.82.66h.15Zm-2.2-4.7c.98 0 1.75.78 1.75 1.75s-.78 1.75-1.75 1.75S16 9.73 16 8.75s.78-1.75 1.75-1.75Z M9.5 16.5a1.5 1.5 0 1 1 0-3 1.5 1.5 0 0 1 0 3Z"/></svg>
                            <span>ETA: {formatTime(etaCurrent)}</span>
                        </div>
                    ) : (
                        <div className="eta-prompt">
                            <span>Check ETA</span>
                        </div>
                    )}
                </div>
            </header>

            <div className="transcript-container" ref={transcriptRef}>
                {messages.map((msg, index) => (
                    <div key={index} className={`message-bubble ${msg.sender.toLowerCase()}-message`}>
                        {msg.text}
                    </div>
                ))}
                 {isProcessing && !isListening && messages.length > 1 && (
                    <div className="message-bubble emd-message typing-indicator">
                        <span></span><span></span><span></span>
                    </div>
                )}
            </div>

            <footer className="call-footer">
                <button className="hangup-button" onClick={() => onEndCall({ stressTaps })} aria-label="Hang up call">
                    END
                </button>
                <button 
                    className={`mic-button ${isListening ? 'listening' : ''}`} 
                    onClick={toggleListen}
                    disabled={isProcessing}
                    aria-label={isListening ? 'Stop listening' : 'Start listening'}
                >
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24" fill="currentColor"><path d="M12 14q.825 0 1.413-.588T14 12V6q0-.825-.588-1.413T12 4q-.825 0-1.413.588T10 6v6q0 .825.588 1.413T12 14Zm-1 7v-3.075q-2.6-.35-4.3-2.325T5 11H7q0 2.075 1.463 3.538T12 16q2.075 0 3.538-1.463T17 11h2q0 2.225-1.7 4.2T13 17.925V21h-2Z"/></svg>
                </button>
            </footer>
        </div>
    );
};

const container = document.getElementById('root');
if (container) {
    const root = createRoot(container);
    root.render(
        <React.StrictMode>
            <div className="app">
                <App />
            </div>
        </React.StrictMode>
    );
}